{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea08781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shlok\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Clean Answer:\n",
      "\n",
      "Members have access to thousands of books, training videos, Learning Paths, interac‚Äê tive tutorials, and curated playlists from over publishers, including O‚ÄôReilly Media, Harvard Business Review, Prentice Hall Professional, Addison Wesley Profes‚Äê sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw Hill, Jones & Bartlett, and Course Technology, among others. For more information, please visit http://oreilly.com/safari. How to Contact Us Please address comments and questions concerning this book to the publisher: O‚ÄôReilly Media, Inc. Gravenstein Highway North Sebastopol, CA (in the United States or Canada) (international or local) (fax) We have a web page for this book, where we list errata, examples, and any additional your current area of expertise. Whether you are reporting election results, forecasting stock returns, optimizing online ad clicks, identifying microorganisms in microscope photos, seeking new classes of astronomical objects, or working with data in any other field, the goal of this book is to give you the ability to ask and answer new ques‚Äê tions about your chosen subject area. Who Is This Book For? In my teaching both at the University of Washington and at various tech focused conferences and meetups, one of the most common questions I have heard is this: ‚Äúhow should I learn Python?‚Äù The people asking are generally technically minded students, developers, or researchers, often with an already strong background in writ‚Äê ing code and using computational and numerical tools. Most of these folks don‚Äôt want to learn Python per se, but want to learn the language with the aim of using it as a tool for data intensive and computational science. While a large patchwork of videos, colleague with a computer problem, most of the time it‚Äôs less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it‚Äôs the same: searchable web resources such as online documentation, mailing list threads, and Stack Overflow answers contain a wealth of information, even (espe‚Äê cially?) if it is a topic you‚Äôve found yourself searching before. Being an effective prac‚Äê titioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the informa‚Äê tion you don‚Äôt know, whether through a web search engine or another means. One of the most useful functions of IPython/Jupyter is to shorten the gap between the user and the type of documentation and search that will help them do their work effectively. While web searches still play a role in answering complicated questions, an amazing amount of information can be found through IPython alone. Some\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os, re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatMistralAI(\n",
    "    api_key=api_key,\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Load and process PDF\n",
    "file_path = \"C:\\\\Users\\\\shlok\\\\Desktop\\\\ML_Referenes\\\\Python_Datascience.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "doc = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "chunks_of_doc = splitter.split_documents(doc)\n",
    "print(f\"Total chunks created: {len(chunks_of_doc)}\")\n",
    "\n",
    "# Clean encoding\n",
    "def clean_text(text):\n",
    "    return text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "cleaned_chunks = [\n",
    "    Document(page_content=clean_text(doc.page_content), metadata=doc.metadata)\n",
    "    for doc in chunks_of_doc\n",
    "]\n",
    "\n",
    "# Embedding + FAISS\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "vectorstore = FAISS.from_documents(cleaned_chunks, embeddings)\n",
    "\n",
    "# User Question\n",
    "question = \"what is the work experience\"\n",
    "response = vectorstore.similarity_search(question, k=3)\n",
    "\n",
    "# Clean answer formatting\n",
    "def clean_output(text):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join([doc.page_content for doc in text])\n",
    "    text = re.sub(r'[\\n`*‚Ä¢#\\-\\d]+\\.*\\s*', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# RAG Chain\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "# Get Final Answer from LLM\n",
    "final_answer = chain.run(question)\n",
    "\n",
    "# Output\n",
    "print(\"\\nüß† Final Clean Answer:\\n\")\n",
    "print(clean_output(final_answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1664575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
